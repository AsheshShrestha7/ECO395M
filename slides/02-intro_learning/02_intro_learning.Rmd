---
title: "Principles of statistical learning"
author: "James Scott"
date: "ECO 395M: Data Mining and Statistical Learning"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(fig.width = 4, fig.asp=0.6, size='footnotesize')
```

## Outline

1. Introduction  
2. Parametric vs nonparametric models: KNN    
3. Measuring accuracy  
4. Out-of-sample predictions  
5. Train/test splits  
6. Bias-variance tradeoff  
7. A more careful look at measuring model accuracy  

## Introduction to predictive modeling  

The goal is to predict a target variable ($y$) with feature variables ($x$).  

- Zillow: predict price ($y$) using a house's features ($x$ = size, beds, baths, age, ...)  
- Citadel: predict next month's S&P ($y$) using this month's economic indicators ($x$ = unemployment, GDP growth rate, inflation, ...)  
- MD Anderson: predict a patient's disease progression ($y$) using his or her clinical, demographic, and genetic indicators ($x$)  
- Etc.

In data mining/ML/AI, this is called "supervised learning."  In intro Prob/Stats, we saw a simple example (OLS with one $x$ feature)  


## Introduction to predictive modeling   


A useful way to frame this problem is to postulate that:  

$$
y_i = f(x_i) + e_i  
$$

- $y_i$ is a numerical _outcome_ or _target_ variable  
- $x_i = (x_{i1}, x_{i2}, ... x_{iP})$ is a vector of features
- $f$ is an unknown function    
- each pair $(x_i, y_i)$ is called an "example"  

Our main purpose is to:  

- _learn_ $f(x)$ from the observed data  
- make predictions on new $x$ points, $\hat{y} = f(x)$.  

Note: this is regression; _classification_ (when $y$ is a categorical rather than numerical) is a similar problem that we'll discuss later.  

## Example: predicting electricity demand

```{r, out.width = "250px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/ERCOTOperator_2.jpg")
```

ERCOT operates the electricity grid for 75% of Texas by area.  


## Example: predicting electricity demand

:::::: {.columns}

::: {.column width="55%"}
```{r, out.width = "150px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/ercot_regions.png")
```

:::

::: {.column width="45%"}


The 8 ERCOT regions are shown at left.  We'll focus on a basic prediction task:  

- $y$ = demand (megawatts) in the Coast region at 3 PM, every day from 2010-2016.  
- $x$ = average daily temperature  at Houston's Hobby Airport (degrees Celsius)

:::

::::::


## Demand versus temperature

```{r, message=FALSE, echo=FALSE}
library(tidyverse)
loadhou = read.csv('loadhou.csv')
```

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
g0 = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey', alpha=0.5) + 
  ylim(7000, 20000)
g0
```


## A linear model?

```{r, echo=FALSE, message=FALSE, include=FALSE}
lm1 = lm(COAST ~ KHOU, data=loadhou)
lm1_pred = function(x) {
  predict(lm1, newdata=data.frame(KHOU=x))
}
```

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
g0 + stat_function(fun=lm1_pred, color='red', size=2)
```

$$
  f(x) = \beta_0 + \beta_1 x
$$


## A quadratic model?
  
  
```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
lm2 = lm(COAST ~ poly(KHOU, 2), data=loadhou)
lm2_pred = function(x) {
  predict(lm2, newdata=data.frame(KHOU=x))
}
g0 +  stat_function(fun=lm2_pred, color='red', size=2)
```

$$
  f(x) = \beta_0 + \beta_1 x + \beta_2 x^2
$$
  
  
## How about this model?
  
  
```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
library(caret) 
library(foreach)
ytrain = loadhou$COAST
Xtrain = data.frame(KHOU=jitter(loadhou$KHOU))

knn50 = knnreg(Xtrain, ytrain, k=50)
knn50_pred = function(x) {
  predict(knn50, newdata=data.frame(KHOU=x))
}

g0 + stat_function(fun=knn50_pred, color='red', size=1, n=1001)
```

We can't write down an equation for this $f(x)$.  But we can define it by its behavior! (If $x = 15$, what is $f(x)$?  What about if $x = 30$?)  


  
## How do we estimate f?


Our _training data_ consists of pairs

$$
D_{\mbox{tr}} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}
$$

We then use some statistical method to estimate $f(x)$.  Here "statistical" just means "we apply some recipe to the data."

There are two general families of strategy.

- Parametric models: assume a particular, restricted functional form (e.g. linear, quadratic, logs, exp)
- nonparametric models: flexible forms not easily described by simple math functions.  (Requires you to give up the grade-school idea that a function is an equation you can write down.)  




## A quick comparison


:::::: {.columns}

::: {.column width="50%"}

```{r, fig.width=2, fig.asp = 0.8, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
g0 +  stat_function(fun=lm2_pred, color='red', size=2) + 
  theme_bw(base_size=8)
```

Parametric:

- polynomial model  
- $f(x) = \beta_0 + \beta_1 x + \beta_2 x^2$

:::

::: {.column width="50%"}

```{r, fig.width=2, fig.asp = 0.8, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
g0 +  stat_function(fun=knn50_pred, color='red', size=2) + 
  theme_bw(base_size=8)
```

Nonparametric:  

- k-nearest neighbors  
- $f(x)$ = average $y$ value of the 50 points closest to $x$  

:::

::::::



## Estimating a parametric model: three steps




1. Choose a functional form of the model, e.g.
$$
f(x) = \beta_0 + \beta_1 x
$$

2. Choose a _loss function_ that measures the difference between the model predictions $f(x)$ and the actual outcomes $y$.  E.g. least squares:
$$
\begin{aligned}
L(\beta_0, \beta_1) &= \sum_{i=1}^N (y_i - f(x_i))^2 \\
&= \sum_{i=1}^N (y_i - \{\beta_0 + \beta_1 x_i \})^2
\end{aligned}
$$

3. Find the parameters that minimize the loss function.  


## Estimating k-nearest neighbors


Suppose we have our training data in the form of $(x_i, y_i)$ pairs.  Now we want to predict $y$ at some new point $x^{\star}$.

1. Pick the $K$ points in the training data whose $x_i$ values are closest to $x^{\star}$.  Call this neighborhood $\mathcal{N}_K(x^{\star})$.   
2. Average the $y_i$ values for those points and use this average to estimate $f(x^{\star})$:  
$$  
\hat{f}(x^\star) = \frac{1}{K} \sum_{i: x_i \in \mathcal{N}_K(x^\star)} y_i   
$$

There are __no explicit parameters__ (i.e. $\beta$'s) to estimate.  

- Rather, the estimate for $f(x)$ is defined by a particular _algorithm_ applied to the data set.  
- Note for the mathematically rigorous: $f(x)$ is defined _point-wise_, that is, by applying the same recipe at any point $x$.)



## At x=5

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 5
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

## At x=10


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 10
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

## At x=15

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 15
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

## At x=20

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 20
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

## At x=25

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 25
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```


## At x=30

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 30
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

## At x=35


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
K = 50

x_star = 35
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```



## The predictions across all x values

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
g0 + stat_function(fun=knn50_pred, color='red', size=1, n=1001)
```

## Two questions

This procedure raises two obvious questions:

1. So why average the nearest $K=50$ neighbors?  Why not $K=2$, or $K=200$?
2. And if we're free to pick any value of $K$ we like, how should we choose?

Let's take the true coder's approach: mess with stuff and see what happens!  

## K=2

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=2)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=10) + 
  ylim(7000, 20000)

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

## K=5


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=5)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


## K=10


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=10) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


## K=20

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=20)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}
 
p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


## K=50


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=50)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

## K=100


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=100)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

## K=200


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=200)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

## K=500


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
library(FNN)
Xtest = data.frame(KHOU = seq(-2, 36, by=0.1))

knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 500)
d_test = data.frame(Xtest, ypred = knn_model$pred)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

## K=1000

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 1000)
d_test = data.frame(Xtest, ypred = knn_model$pred)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

## K=1500


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 1500)
d_test = data.frame(Xtest, ypred = knn_model$pred)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

## K=2000


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 2000)
d_test = data.frame(Xtest, ypred = knn_model$pred)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

## K=2357


```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 2357)
d_test = data.frame(Xtest, ypred = knn_model$pred)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```


## Complexity, generalization, and interpretion

Smaller values of $K$ give more flexible, but less stable function estimates: 

- they can capture very fine-scale structure in $f(x)$, because they're only averaging points from a small neighborhood...
- but they can also confuse noise for signal!  

Larger values of $K$ give less flexible, but more stable function estimates: 

- they can't adapt as much to wiggles in $f(x)$, because they're averaging points over a larger neighborhood.    
- but this makes them less prone to confusing noise for signal.    

You probably got the sense from the pictures that there's a "happy medium" somewhere.  __How should we find it?__  


## Measuring accuracy

Let's go with a simple principle: choose the model that makes the most accurate predictions, on average.  

A standard measure of (in)accuracy is the root mean-squared error:  
  $$
  \mathrm{RMSE}_{\mathrm{in}}= \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2 }
  $$
    
This measures, on average, how large are the errors made by the model on the training data.  (OLS minimizes this quantity over the set of linear functions.)  

RMSE is just one possible error metric, but it's pretty popular.    (You'll also see RMSE without the square root, or MSE, but it's hard to interpret.)      
  

## Measuring accuracy: linear vs. quadratric
  
  
  ```{r, fig.width = 4, fig.asp = 0.6, fig.align='center', echo=FALSE, warning=FALSE}
  lm1 = lm(COAST ~ KHOU, data=loadhou)
  lm2 = lm(COAST ~ poly(KHOU, 2), data=loadhou)
  rmse = function(y, ypred) {
    sqrt(mean((y-ypred)^2))
  }
  
  rmse_lm1 = rmse(ytrain, predict(lm1))
  rmse_lm2 = rmse(ytrain, predict(lm2))
  
  d_test1 = data.frame(Xtest, ypred = predict(lm1, Xtest))
  d_test2 = data.frame(Xtest, ypred = predict(lm2, Xtest))
  
  p_base + geom_path(data=d_test1, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
  ```
  
  $$
    RMSE = 1807
  $$
    


## Measuring accuracy: linear vs. quadratric
  
```{r, fig.width = 4, fig.asp = 0.6, fig.align='center', echo=FALSE, warning=FALSE}
p_base + geom_path(data=d_test2, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```
  
  $$
    RMSE = 1001
  $$
    
    

    
## Measuring accuracy: RMSE versus K for KNN
    
    

```{r, fig.width = 4, fig.asp = 0.6, fig.align='center', echo=FALSE, warning=FALSE}
k_grid = unique(round(exp(seq(log(2000), log(2), length=100))))
rmse_grid_in = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knn.reg(Xtrain, Xtrain, ytrain, k = k)
  rmse(ytrain, knn_model$pred)
}
  
  revlog_trans <- function(base = exp(1)) {
    require(scales)
    ## Define the desired transformation.
    trans <- function(x){
      -log(x, base)
    }
    ## Define the reverse of the desired transformation
    inv <- function(x){
      base^(-x)
    }
    ## Creates the transformation
    scales::trans_new(paste("revlog-", base, sep = ""),
                      trans,
                      inv,  ## The reverse of the transformation
                      log_breaks(base = base), ## default way to define the scale breaks
                      domain = c(1e-100, Inf) 
    )
  }
  
  rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)
  ggplot(data=rmse_grid_in) + 
    geom_path(aes(x=K, y=RMSE)) + 
    labs(y = "RMSE (in-sample)") +
    scale_x_continuous(trans=revlog_trans(base = 10)) + 
    geom_hline(yintercept=1001, color='blue', size=1) + 
    geom_hline(yintercept=1807, color='red', size=1)
```
  
  
  
## So we should pick K=2?  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=2)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=10) + 
  ylim(7000, 20000)

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1000)
```

\center RMSE = 670
  
  
## So we should pick K=2? Ask Yogi!

   
```{r, out.width = "250px", fig.align='center', echo=FALSE}
  knitr::include_graphics("fig/berra0.jpg")
```

## So we should pick K=2? Ask Yogi!

    
```{r, out.width = "250px", fig.align='center', echo=FALSE}
  knitr::include_graphics("fig/berra1.jpg")
```
  
## So we should pick K=2? Ask Yogi!

```{r, out.width = "250px", fig.align='center', echo=FALSE}
  knitr::include_graphics("fig/berra2.jpg")
```
  
## So we should pick K=2? Ask Yogi!

```{r, out.width = "250px", fig.align='center', echo=FALSE}
  knitr::include_graphics("fig/berra3.jpg")
```
  
## So we should pick K=2? Ask Yogi!

```{r, out.width = "250px", fig.align='center', echo=FALSE}
  knitr::include_graphics("fig/berra4.jpg")
```



## Out-of-sample accuracy

    
Making good predictions about the past isn't very impressive.  

Our very complex (K=2) model earned a low RMSE by simply memorizing the random pattern of noise in the training data.

It's like getting a perfect score on the GRE when someone tells you what the questions are ahead of time: it doesn't predict anything about how well you'll do on the __next__ test.    
  

  
## Out-of-sample accuracy

    
Key idea: what really matters is our prediction accuracy out-of-sample!
    
Suppose we have $M$ __additional__ observations $(x_i^{\star}, y_i^{\star})$ for $i = 1, \ldots, M$.

- The important thing is that these are data points that we _did not use_ to fit the model.  
- We'll call this the "testing" data, to distinguish it from our original ("training") data.  

The out-of-sample root mean-squared error is then:  
$$
    \mathrm{RMSE}_{\mathrm{out}} = \sqrt{ \frac{1}{M} \sum_{i=1}^M (y_i^{\star} - f(x_i^{\star}))^2 }
$$
    
    
## Using a train/test split
  
    
We don't have any "future data" to use to test our model.  We just have our $N$ original data points.  

A simple solution is a train/test split.  That is, we split our data set $D$ into two subsets:

- A training set $D_{in}$ of size $N_{in} < N$, to use for fitting the models under consideration.  
- A testing set $D_{out}$ of size $N_{out}$.
- $D = D_{in} \cup D_{out}$ and $N = N_{in} + N_{out}$, but $D_{in} \cap D_{out} = \emptyset$.  
- No cheating!  We use $D_{in}$ _only_ to fit the models, and $D_{out}$ _only_ to compare the out-of-sample accuracy of the models.  

## Using a train/test split

The R code for this is pretty simple.  For example, here's out-of-sample RMSE calculated for KNN-25 model:  

\medskip

```{r, echo=FALSE, message=FALSE}
library(rsample)
library(caret)
library(foreach)
library(modelr)
loadhou = read.csv('loadhou.csv')
```

\footnotesize
```{r, echo=TRUE}
loadhou_split =  initial_split(loadhou, prop=0.8)
loadhou_train = training(loadhou_split)
loadhou_test  = testing(loadhou_split)
  
# train the model and calculate RMSE on the test set
knn_model = knnreg(COAST ~ KHOU, data=loadhou_train, k = 25)
modelr::rmse(knn_model, loadhou_test)
```
\normalsize 

So let's check performance across a variety of choices for K, versus the linear and quadratic models. 

## Linear model: train

```{r,include=FALSE}
N = nrow(loadhou)
N_train = floor(0.8*N)
train_ind = sort(sample.int(N, N_train, replace=FALSE))

D_all = loadhou; D_all$set = 'test'; D_all$set[train_ind] = 'train'
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
```

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000)
  
# Fit the model
lm1 = lm(COAST ~ KHOU, data=D_train)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm1, D_plot)

p_train + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```


## Linear model: test

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
p_test = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  ylim(7000, 20000)
  
# Predict
y_pred = predict(lm1, D_test)
lm1_rmse = rmse(D_test$COAST, y_pred)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm1, D_plot)

p_test + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```

$$
RMSE_{out} = `r round(lm1_rmse, 0)`
$$



## Quadratic model: train

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
# Fit the model
lm2 = lm(COAST ~ poly(KHOU,2), data=D_train)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm2, D_plot)

p_train + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```


## Quadratic model: test

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
# Predict
y_pred = predict(lm2, D_test)
lm2_rmse = rmse(D_test$COAST, y_pred)

# Plot
p_test + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```

$$
RMSE_{out} = `r round(lm2_rmse, 0)`
$$


## K-nearest neighbors: test  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
D_test = arrange(D_test, KHOU)
y_train = D_train$COAST
y_test = D_test$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))
X_test = data.frame(KHOU=D_test$KHOU)

k_grid = unique(round(exp(seq(log(1500), log(2), length=100))))
rmse_grid_out = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knn.reg(X_train, X_test, y_train, k = k)
  rmse(y_test, knn_model$pred)
}

rmse_grid_out = data.frame(K = k_grid, RMSE = rmse_grid_out)

p_out = ggplot(data=rmse_grid_out) + 
  theme_bw(base_size = 10) + 
  geom_path(aes(x=K, y=RMSE, color='testset'), size=0.5) + 
  scale_x_continuous(trans=revlog_trans(base = 10)) + 
  geom_hline(yintercept=lm2_rmse, color='blue', size=1) + 
  geom_hline(yintercept=lm1_rmse, color='red', size=1)

ind_best = which.min(rmse_grid_out$RMSE)
k_best = k_grid[ind_best]

p_out + geom_path(data=rmse_grid_in, aes(x=K, y=RMSE, color='trainset'),size=0.5) + 
  ylim(700, 2000) +
  scale_colour_manual(name="RMSE",
    values=c(testset="black", trainset="grey")) + 
  geom_vline(xintercept=k_best, color='darkgreen', size=1)
```

Not too simple, not too complex... This plot illustrates the __bias-variance trade-off__, one of the key ideas of this course.



## K-nearest neighbors: at the optimal k  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knn.reg(X_train, X_test, y_train, k = k_best)
rmse_best = rmse(y_test, knn_model$pred)

D_test$ypred = knn_model$pred
p_test + geom_path(data=D_test, mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

$$
\mathrm{RMSE}_{out} = `r round(rmse_best, 0)`
$$



## Take-home lessons


- In general, $RMSE_{out} > RMSE_{in}$.  That is, the estimate of RMSE from the training set is an over-optimistic assessment of how big your errors will be for future data.  
- For very complex models, $RMSE_{in}$ can be _wildly_ optimistic.  
- The best model is usually one that balances simplicity with explanatory power.  
- Estimating $RMSE_{out}$ using a train-test split of the original data set is a simple way to help us from going too far wrong.  



## In-class exercise

Download the `loudhou` data set and starter R script. Get a feel for how the code behaves:  

1. Make a train/test split.   
2. Train on the training set.   
3. Predict on the testing set.   
4. Plot the results.   
    

## In-class exercise


Then make an informal investigation of the _bias_ and _variance_ of the KNN estimator:   

1. Repeatedly take bootstrap samples of the full data set, and train a $K=3$ model on each small training set.  Plot the fit to the training set.  How stable are they from sample to sample?  How do they behave at the endpoints, i.e. at very low and very high temperatures?  
2. Now do the same thing, except training a $K=75$ model on each bootstrap sample.  How stable are the estimates from sample to sample?  And how do they behave at the endpoints?   

Hint: keep the x and y limits constant across plots, e.g. by adding the layers  `+ ylim(7000, 20000) + xlim(0,36)` or whatever limits seem appropriate.




## K=3: sample 1

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)

p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

## K=3: sample 2

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

## K=3: sample 3

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

## K=75: sample 1

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 75)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

## K=75: sample 2

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 74)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

## K=75: sample 3

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
N_train = nrow(loadhou)
train_ind = sort(sample.int(N, N_train, replace=TRUE))
D_train = loadhou[train_ind,] 
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 75)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000) + xlim(-2, 36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```



## Bias-variance trade-off

High K = high bias, low variance:    

- We estimate $f(x)$ using many points, some of which might be far away from $x$. These far-away points bias the prediction; their values of $f(x)$ are slightly off on average.  
- But more data points means lower variance---less chance of memorizing random noise.    

Low K = low bias, high variance:    

- We estimate $f(x)$ using only points that are _very close_ to $x$.  Far-away $x$ points don't bias the prediction with their "slightly off" $y$ values.  
- But fewer data points means higher variance---more chance of memorizing random noise.    


## Bias-variance trade-off
  
Let's take a deeper look at prediction error. 

- Let $\{(x_1, y_1), \ldots, (x_n, y_n)\}$ be your training data.  
- Suppose that $y = f(x) + e$, where $E(e) = 0$ and $\mbox{var}(e) = \sigma^2$.  
- Let $\hat{f}(x)$ be the function estimate from your training data.  
- Let $x^{\star}$ be a new $x$ point, and let $y^{\star}$ be the corresponding outcome.  
- $x^{\star}$ is fixed (not a random variable), but the training data and the future outcome $y^{\star}$ are both random.    

Define the expected squared prediction error at $x^{\star}$ as:  
$$
MSE^{\star} = E\left\{ \left( y^{\star} - \hat{f} (x^{\star}) \right)^2 \right\} 
$$



## Bias-variance trade-off

For any random variable A, $E(A^2) = \mbox{var}(A) + E(A)^2$.  So:  

$$
\begin{aligned}
MSE^{\star} &= E\left\{ \left( y^{\star} - \hat{f} (x^{\star}) \right)^2 \right\} \\
  &= \mbox{var} \left\{ y^{\star} - \hat{f} (x^{\star}) \right\} + \left( E \left\{ y^{\star} - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \mbox{var} \left\{ f(x^\star) + e^\star - \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) + e^\star - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \sigma^2 + \mbox{(Estimation variance)} + \mbox{(Squared estimation bias)}
  \end{aligned}
$$


## Bias-variance trade-off

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

First, consider $\sigma^2$.  

- This is the intrinsic variability of the data: remember, $y = f(x) + e$, and $\mbox{var}(e) = \sigma^2$.  
- How can we make this term smaller?  


## Bias-variance trade-off

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

Next, consider $\mbox{var} \left\{ \hat{f} (x^{\star}) \right\}$.  

- This is the variance of our estimate $\hat{f}(x)$: remember, our estimate is random, because the data is random.  
- How can we make this term smaller?  


## Bias-variance trade-off

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

Finally, consider $\left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2$.

- This is the bias of our estimate $\hat{f}(x)$: remember, our estimate doesn't necessarily equal the true $f(x)$, even on average.  
- How can we make this term smaller?  
  
  
## Bias-variance trade-off
  
  
That's why it's a trade-off!  

- Smaller estimation variance generally requires a _less complex_ model---intuitively, one that "wiggles less" from sample to sample. (Think K=75 on our `loadhou` example.)  
- Smaller bias generally requires a _more complex_ model---one that can "wiggle more," to adapt to the true function.  (Think K=3 on our `loadhou` example.)  
- Models that "wiggle more" can adapt to more kinds of functions, but they're also more prone to memorizing random noise.    

__Much of the rest of the semester is about finding estimates with the right amount of wiggle!__


## Measuring model accuracy, revisited  

Recall our definition of "true" out-of-sample MSE:  
$$
\mathrm{MSE}^{\star} = E\left\{ \left( y^{\star} - \hat{f} (x^{\star}) \right)^2 \right\} 
$$


As we've seen, a simple way to estimate this quantity is to train our model $\hat{f}$ on $D_{in}$ and to calculate average performance on $D_{out}$:  

$$
\widehat{\mathrm{MSE}} = \frac{1}{N_{\mathrm{out}}} \sum_{i=1}^{N_{\mathrm{out}}} \left( y_i - \hat{f}(x_i) \right)^2
$$


The key word here is __estimate.__  There are two sources of randomness in our estimate:

- $\hat{f}(x)$, the function estimate from $D_{in}$.  
- the specific $(x_i,  y_i)$ pairs that end up in $D_{out}$.   


## Measuring model accuracy, revisited

So let's see what happens if we try this process for ten different random train/test splits:  

\bigskip

\footnotesize
```{r, echo=TRUE, R.options = list(width = 45)}
rmse_out = foreach(i=1:10, .combine='c') %do% {
  loadhou_split =  initial_split(loadhou, prop=0.8)
  loadhou_train = training(loadhou_split)
  loadhou_test  = testing(loadhou_split)
  
  # train the model and calculate RMSE on the test set
  knn_model = knnreg(COAST ~ KHOU, data=loadhou_train, k = 25)
  modelr::rmse(knn_model, loadhou_test)
}
rmse_out
```
\normalsize



## Measuring model accuracy, revisited

Let's see this across multiple values of $K$.

```{r, echo=FALSE, message=FALSE, include=FALSE}
library(parallel)
k_grid = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300)
rmse_out = foreach(i=1:20, .combine='rbind') %dopar% {
  loadhou_split =  initial_split(loadhou, prop=0.8)
  loadhou_train = training(loadhou_split)
  loadhou_test  = testing(loadhou_split)
  this_rmse = foreach(k = k_grid, .combine='c') %do% {
    # train the model and calculate RMSE on the test set
    knn_model = knnreg(COAST ~ KHOU, data=loadhou_train, k = k, use.all=TRUE)
    modelr::rmse(knn_model, loadhou_test)
  }
  data.frame(k=k_grid, rmse=this_rmse)
}
rmse_out = arrange(rmse_out, k)
```

```{r, echo=FALSE, message=FALSE}
ggplot(rmse_out) + geom_boxplot(aes(x=factor(k), y=rmse)) + theme_bw(base_size=7)
```
\normalsize


## Measuring model accuracy, revisited

This is a big lesson: any estimate of RMSE is subject to error!  

- Our $\widehat{\mathrm{RMSE}}$ differs from one train/test split to the next.  
- This is intuitive: $\widehat{\mathrm{RMSE}}$ just an estimate of something unknown ($\mathrm{RMSE}^\star$), and all estimates of unknown quantities have variance.  
- In fact, the variability of $\widehat{\mathrm{RMSE}}$ across different train/test splits _for fixed K_ can be pretty large, compared to the differences across a wide range of K.  

Before we selected $K$ using a single train/test split and picked the $K$ with the lowest RMSE.  __Should we modify our approach?__  


## Averaging across multiple splits  

An obvious solution:  

- Just average $\widehat{\mathrm{RMSE}}$ for all your models across multiple train/test splits.  
- This will reduce the variance of $\widehat{\mathrm{RMSE}}$, compared with using a single train/test split.  
- This is perfectly valid, and we'll sometimes use it when training the model is pretty cheap.  
\pause

But while perfectly sensible, this obvious solution is a bit inefficient.  

- Two randomly sampled testing sets can overlap a lot.
- Overlap means that $\widehat{\mathrm{RMSE}}_1$ and $\widehat{\mathrm{RMSE}}_2$ calculated from these two testing sets are highly correlated.  
- Remember: averaging quantities with high covariance is less efficient than averaging those with low covariance.


## K-fold cross validation

A more efficient solution is $K$-fold cross-validation:  

1. Randomly divide the data set into $K$ nonoverlapping groups, or _folds_, of roughly equal size.  

2. For fold k = 1 to K:  

   - Fit the model using all data points __not in__ fold $k$.  
   - For all points $(y_i, x_i)$ __in__ fold $k$, predict $\hat{y_i}$ using the fitted model.  
   - Calculate $\widehat{\mathrm{RMSE}}_k$, the average error on fold $k$.  

3. Calculate the cross-validated error rate as:
$$
\mathrm{CV}_{(K)} = \frac{1}{K} \sum_{k=1}^K \widehat{\mathrm{RMSE}}_k
$$

Let's draw a picture to build intuition for this procedure.    


## K-fold cross validation

```{r, out.width = "280px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/K-fold-cross-validation-method.png")
```

The split of the data into folds is still random, but in a way that minimizes the overlap between each test set.  

## K-fold cross validation

A few notes:  

- Typical values of $K$ are 5 or 10.  There's not a lot of deep theory here; those are just values that work well in practice.  
- All candidate models should be fit on the same set of folds.  (That is, don't create a different split to evaluate different models.)
- The $K$ in K-fold cross validation is not the same $K$ as the $K$ in $K$-nearest neighbors.  (The only thing they share in common is that K is just generic notation for an integer.)  
- If $K = N$, i.e. the size of the data set, the resulting procedure is called "leave-one-out" cross validation (LOOCV).    We'll talk about this later.  

Let's go back to `loadhou.R` to see cross validation in action.  


## K-fold cross validation

There are two typical ways to select a model using cross validation:  

1. The "min" rule: choose the model with the best cross-validated error.  
2. The "1SE" rule: choose the simplest model whose cross-validated error is within one standard error of the minimum.

For each model, we estimate the standard error of that model's cross-validated RMSE as:

$$
\mathrm{std err} \approx \frac{\mathrm{sd}\left( \widehat{\mathrm{RMSE}}_1, \widehat{\mathrm{RMSE}}_2, \ldots, \widehat{\mathrm{RMSE}}_K \right)}{\sqrt{K}}
$$

(A decent approximation even though the $\widehat{\mathrm{RMSE}}_k$'s aren't IID.)  


## Summary  

Nonparametric models (like KNN):  

- can't be written down in terms of simple math functions  
- can adapt to complex functions, but have to be reined in somehow, so that they don't overfit  
- usually have "tuning" parameters (like K in KNN) that govern the trade-off between bias and variance.  

Out-of-sample performance is the true test of a model:  

- In-sample RMSE is too optimistic, often wildly so.  
- Cross-validation using K=5 or K=10 is a practical way to quantify out-of-sample performance.  
- For close calls, the 1SE rule is a widely accepted way to actually pick the model.  _Simplicity is a virtue_, and the 1SE rule finds the simplest model that doesn't forfeit any statistically noticeable gains in performance.  

