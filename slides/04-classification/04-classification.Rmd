---
title: "Classification"
author: "James Scott"
date: "ECO 395M: Data Mining and Statistical Learning"
urlcolor: blue
output:
  beamer_presentation
---

```{r setup, include=FALSE}
set.seed = 12345678
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(modelr)
library(rsample)
```


## Outline


1. Linear probability model  
2. Logistic regression: the basics  
3. Interpreting a logit model  
4. Estimating a logit model: MLE  
5. Error rates and ROC curves
6. KNN for classification  
7. The multinomial logit model, a.k.a "softmax"
8. Evaluating a classifier: likelihood and deviance  
9. Naive Bayes classification  


## Classification


In classification, the target variable $y$ is membership in a category. 

- occupation: butcher, baker, candlestick maker... 
- consumer choices: Hyundai, Toyota, Ford... 
- college major: economics, mathematics, literature...  
- tumor type: malignant or benign  
- political slant of a new article: R or D  

Each observation consists of:  

- an observed class $y_i \in \{1, \ldots, M\}$  
- a vector of features $x_i$.   

The classification problem: given new $x^{\star}$, predict $y^{\star}$ (or provide $P(y^\star = k)$ for each class $k$).  


## Linear probability model


We'll start with binary classification (where we label the outcomes $y$ as 0 for no or 1 for yes, appropriately defined in context).    

Recall the basic form of a supervised learning problem:  
$$
E(y \mid x) = f(x)  
$$

If $y$ is binary (0/1), then:
$$
\begin{aligned}
E(y \mid x) &= 0 \cdot P(y = 0 \mid x) + 1 \cdot P(y = 1 \mid x) \\
&= P(y = 1 \mid x)  
\end{aligned}
$$

Conclusion: the expectation of a binary outcome is the probability that the outcome is 1.  


## Linear probability model


Suppose we choose $f(x)$ to be a linear function of the features $x_i$:
$$
\begin{aligned}
P(y = 1 \mid x) &= E(y \mid x) \\
&= x \cdot \beta \\
& = \beta_0 + \sum_{j=1}^p x_{ij} \beta_j \\
\end{aligned}
$$

This is called the _linear probability model_: the probability of a "yes" outcome ($y=1$) is linear in $x_i$.  To fit this, we:  

- Code our outcomes $y_i$ as a dummy variable.  
- Throw them into a linear regression model and pretend they're numbers!  
- The resulting model predictions give us fitted probabilities.  


## LPM: spam classification

![](fig/spam.jpeg){width=100%}


Let's consider a simple spam classification problem:  

- `spamfit.csv`: 3000 e-mails (40% spam) with 9 features.  
- `spamtest.csv`: 601 testing e-mails for assessing performance.  

## LPM: spam classification


Here are the first few lines of our testing data.

\bigskip

\tiny
```{r, message=FALSE, echo=TRUE, R.options = list(width=80)}
spamfit = read.csv('../data/spamfit.csv')
spamtest = read.csv('../data/spamtest.csv')

# first few lines
head(spamtest, 3)
```
\normalsize



## LPM: spam classification



Let's build a linear probability using all the available features for P(spam | x) and examine the fitted coefficients:  

\footnotesize
```{r}
# Recall: the dot (.) says "use all variables not otherwise named"
lm_spam1 = lm(y ~ ., data=spamfit)
coef(lm_spam1) %>% round(3)
```


## LPM: spam classification


Let's look at our in-sample performance by:  

- thresholding our predicted probabilities at 0.5  
- calculating the __confusion matrix__, which tabulates predicted class versus actual class.  

\bigskip

\footnotesize
```{r}
phat_train_spam1 = predict(lm_spam1, spamfit)
yhat_train_spam1 = ifelse(phat_train_spam1 > 0.5, 1, 0)
confusion_in = table(y = spamfit$y, yhat = yhat_train_spam1)
confusion_in
sum(diag(confusion_in))/sum(confusion_in)  # in-sample accuracy
```

## LPM: spam classification

Let's do the same with out-of-sample performance:

\bigskip

\footnotesize
```{r}
phat_test_spam1 = predict(lm_spam1, spamtest)
yhat_test_spam1 = ifelse(phat_test_spam1 > 0.5, 1, 0)
confusion_out = table(y = spamtest$y, yhat = yhat_test_spam1)
confusion_out  # confusion matrix
sum(diag(confusion_out))/sum(confusion_out)  # out-of-sample accuracy
```

## LPM: spam classification


How well is our model doing?  To answer this question, it helps to have a baseline.  

Note that 60% of the training set isn't spam:
```{r}
table(spamfit$y)
```

Since ``not spam" is the most likely outcome, a reasonable baseline or "null model" is one that guesses "not spam" for every test-set instance.



## LPM: spam classification

How well does this null model perform on the test set?  About 64\%, since it gets all the 0's right and 1's wrong:

\footnotesize
```{r}
table(spamtest$y)
385/sum(table(spamtest$y))
```
\normalsize

Our linear probability model had an 81.5% out-of-sample accuracy rate.  Therefore, compared to the null model:  

- Its absolute improvement is $\approx 81.5 - 64.1 = 17.4\%$.   
- Its relative improvement, or _lift_, is $\approx 81.5/64.1 = 1.27$.  


## Take-home lessons

To get predicted classes from a model, we often have to threshold predicted probabilities.   

- Seems like 0.5 is a reasonable baseline.  
- But other thresholds might be appropriate for some problems.  

To measure the accuracy of the model, we might simply ask about its overall classification accuracy ("0/1 loss").  But we'll see other model evaluation metrics that are more nuanced.  

Comparing a model to a baseline or "null model" is often an important sanity check, especially in complicated problems.  

- The null model might be one that knows nothing about $x$.   
- OR it might be a very simple model.  


## LPM: illegal probabilities

The linear probability model has one obvious problem: it can produce fitted probabilities that fall outside (0,1).  E.g. here is a histogram of predicted probabilities for the spam test set, where 34/601 predictions (5.6%) have this problem:  

```{r, echo=FALSE, fig.width = 4, fig.asp = 0.65, fig.align='center'}
phat_test_spam1 = predict(lm_spam1, spamtest)
par(mar=c(4,4,4,1))
hist(phat_test_spam1, breaks=100, cex.axis=0.7, cex.main=0.7, cex.lab = 0.7,
     xlim=c(-0.5, 1.5))
abline(v = c(0,1))
```

## LPM: illegal probabilities

This is a bit inelegant, and it happens for a straightforward reason.  

Recall the basic form of the linear probability model:
$$
P(y = 1 \mid x) = x \cdot \beta
$$

The core of the problem is this:  

- the left-hand side needs to be constrained to fall between 0 and 1, by the basic rules of probabilities  
- but the right-hand side is unconstrained -- it can be any real number.  


## Modifying the LPM


A natural fix to this problem is to break our model down into two pieces:
$$
P(y = 1 \mid x) = g(x \cdot \beta)
$$

The inner piece, $f(x) = x \cdot \beta$, is called the _linear predictor_.  It maps features $x_i$ onto real numbers.  

The outer piece, $g(z)$ is called a _link function._

- It links the linear predictor $z_i \equiv f(x_i) = x_i \cdot \beta$ on the right to the probability on the left.  
- It should map real numbers onto the unit interval (0,1).  


## Logistic regression


A standard choice is $g(z) = e^z/(1+e^z)$.  

- At $z = 0$, $g(z) = 0.5$.
- When $z \to \infty$, $g(z) \to 1$, and when $z \to \infty$, $g(z) \to 0$.

\bigskip

```{r, echo=FALSE, fig.width = 4, fig.asp = 0.7, fig.align='center'}
curve(exp(z)/(1+exp(z)), xname='z', from=-5, to=5, ylab='g(z)', las=1,
      cex.axis=0.7, cex.main=0.7, cex.lab=0.7)
abline(h=c(0,1), lty='dotted')
```

## Logistic regression


This is called the "logistic" or "logit" link, and it leads to the logistic regression model:
$$
P(y = 1 \mid x) = \frac{\exp(x \cdot \beta)}{1 + \exp(x \cdot \beta)}
$$

This is a very common choice of link function.  One reason is interpretability: a little algebra shows that
    $$
    \begin{aligned}
    \log \left[ \frac{p}{1-p} \right] &= x \cdot \beta \\
    \frac{p}{1-p} &= e^{x \cdot \beta}
    \end{aligned}
    $$
    so that it is a log-linear model for the  _odds_ of a yes outcome.  
    
    
## Logistic regression is easy in R

The R syntax is nearly identical to `lm`:  

```
glm(y ~ x, data=mydata, family=binomial)
```

`glm` stands for "generalized linear model," i.e. a linear model with a link function.  The argument `family=binomial` tells R that $y$ is binary and defaults to the logit link.  

The response can take several forms:

- `y = 0, 1, 1,...` numeric vector  
- `y = FALSE, TRUE, TRUE,...` logical    
- `y = 'not spam', 'spam', 'spam',...` factor with 2 levels

Everything else is the same as in linear regression!
    
    
    
## Logistic regression in your inbox



Let's fit a logit model to the spam data.

\footnotesize
```{r}
# Recall: the dot (.) says "use all variables not otherwise named"
logit_spam = glm(y ~ ., data=spamfit, family='binomial')
```


\normalsize
We're warned that some emails are clearly spam or not spam.

- This means p = 0 or p=1 up to floating-point numerical precision for at least one training-set example.  
- This `warning` is largely benign and isn't something to worry about.  
- However, if __all or most__ of the predicted probabilities are that extreme, then either your problem is really easy or your model might be overfitting the training data.   


## Logistic regression in your inbox

```{r, echo=FALSE, message=FALSE, fig.width = 4.5, fig.asp = 0.6, fig.align='center', warning=FALSE}
spamtest = mutate(spamtest, yhat = predict(logit_spam, spamtest, type='response'))
ggplot(spamtest) + 
  geom_jitter(aes(x=factor(y), y=yhat), width=0.1, alpha=0.2) + 
  labs(title="Test-set predicted probabilities", y = "P(spam | x)", x="Spam?") + 
  stat_summary(aes(x=factor(y), y=yhat), fun='mean', col='red', size=1)
```


## Interpeting coefficients in LR  

\footnotesize
```{r}
coef(logit_spam) %>% round(2)
```
\normalsize

Recall our model is
$$
\mbox{Odds} = \frac{p}{1-p} = e^{\beta_0} \cdot e^{\beta_1 x_1} \cdots e^{\beta_p x_p}
$$
So $e^{\beta_j}$ is an _odds multiplier_ or _odds ratio_ for for a one-unit increase in feature $x_j$.  


## Interpeting coefficients in LR  



\footnotesize
```{r}
coef(logit_spam) %>% round(2)
```
\normalsize

The $\beta$ for `char.freq.free` is 1.1.  So having an extra `free` in an e-mail multiplies odds of spam by $e^{1.1} \approx 3$.  


## Interpeting coefficients in LR  


\footnotesize
```{r}
coef(logit_spam) %>% round(2)
```
\normalsize

The $\beta$ for `char.freq.semicolon` is -1.7.  So having an extra semicolon in an e-mail multiplies odds of spam by $e^{-1.7} \approx 0.2$.  (Down by a factor of five!  Note to spammers: use more complex syntax.)  




## Interpeting coefficients in LR  

\footnotesize
```{r}
coef(logit_spam) %>% round(2)
```
\normalsize

The $\beta$ for `word.freq.remove` is 5.7.  So having an extra `remove` in an e-mail multiplies odds of spam by $e^{5.7} \approx 300$.  

Q: What is the odds multiplier for a coefficient of 0?



## LR for spam: out-of-sample 

Let's go to the confusion matrix to check our out-of-sample performance.  

\medskip

\footnotesize

```{r, warning=FALSE}
phat_test_logit_spam = predict(logit_spam, spamtest, type='response')
yhat_test_logit_spam = ifelse(phat_test_logit_spam > 0.5, 1, 0)
confusion_out_logit = table(y = spamtest$y,
                            yhat = yhat_test_logit_spam)
confusion_out_logit
```
\normalsize

We did better than the linear probability model:  

- Error rate (51+27)/601 $\approx$ 13%, or accuracy of 87%.  
- Absolute improvement over LPM: $87 - 81.5 = 6.5\%$. 
- Lift over LPM: $87/81.5 \approx 1.07$.


## Estimating a logit model


A logistic regression model is fit by the principle of __maximum likelihood__: _choose the parameters so that the observed data looks as likely as possible._

Many fitting methods in machine learning are based on either maximum likelihood, or very similar principles.  Let's see the details for this model.  


## Estimating a logit model

Recall from Prob/Stat that the likelihood function is the predicted probability of the observed data, as a function of the parameters.

In logistic regression, the likelihood is built from three assumptions/pieces:  

1. The individual outcomes are binary.  
2. The predicted probabilities are related to the model parameters via
$$
\begin{aligned}
P(y_i = 1 \mid x_i) &= \frac{e^{x \cdot \beta}}{1 + e^{x \cdot \beta}} \\
P(y_i = 0 \mid x_i) &= 1- \frac{e^{x \cdot \beta}}{1 + e^{x \cdot \beta}} = \frac{1}{1 + e^{x \cdot \beta}} \\
\end{aligned}
$$
3. Each binary outcome is presumed independent of the others.  

## Estimating a logit model


Let's think about the likelihood for a single observation (the $i$th one).  This answers the question: how likely was it that we saw this particular outcome ($y=0$ or $y=1$) for observation $i$, assuming the true parameter was $\beta$?  

Here's a convenient way to write it:  

$$
L_i(\beta) = \left( \frac{e^{x \cdot \beta}}{1 + e^{x \cdot \beta}} \right)^{y_i} \cdot \left( \frac{1}{1 + e^{x \cdot \beta}} \right)^{1- y_i}
$$

If $y_i = 1$, the second term gets zeroed out.  Similarly, if $y_i = 0$, the first term gets zeroed out.  


## Estimating a logit model


Now we invoke independence.  The overall likelihood is then
$$
L(\beta) = \prod_{i=1}^N L_i(\beta)
$$
or on a log scale, to avoid numerical underflow:
$$
\begin{aligned}
l(\beta) &= \sum_{i=1}^N \log L_i(\beta) \\ 
&= \sum_{i=1}^N \left[ y_i \cdot x_i \cdot \beta - \log(1 + e^{x \cdot \beta}) \right]
\end{aligned}
$$
This quantity can be maximized as a function of $\beta$ using an iterative numerical routine (typically Newton's method, sometimes gradient ascent or BFGS).  Details for another course (feel free to ask me)!  



## Error rates


We can take a slightly more nuanced look at the performance of a classifier than simply calculating an overall accuracy/error rate.

Here are three simple metrics you should know about:  

- true positive rate  (sensivity, recall)
- the false positive rate  (specificity)
- the false discovery rate  (precision, positive predictive value)

Let's see these three in action on our logit classifier for spam.  


## LR for spam: true positive rate


The _true positive rate_ (TPR): among spam e-mails ($y=1$), how many are correctly flagged as spam ($\hat{y} = 1$)?  
```{r, echo=FALSE}
confusion_out_logit
```

Here the out-of-sample TPR is $165/(51+165) \approx 0.76$.  

Synonyms for the TPR: sensitivity, recall.


## LR for spam: false positive rate


The _false positive rate_ (FPR): among non-spam e-mails ($y=0$), how many are wrongly flagged as spam ($\hat{y} = 1$)?  
```{r, echo=FALSE}
confusion_out_logit
```

Here the out-of-sample FPR is $27/(27+358) \approx 0.07$.  

Synonyms: _specificity_ is the opposite of FPR, but conveys same information:

$$
\mbox{Specificity} = 1 - \mbox{FPR}
$$

So this procedure had a 93% out-of-sample specificity.


## LR for spam: false discovery rate

The _false discovery rate_ (FDR): among e-mails flagged as spam ($\hat{y}=1$), how many were actually not spam ($y = 0$)?  
```{r, echo=FALSE}
confusion_out_logit
```

Here the out-of-sample FDR is $27/(27+165) \approx 0.14$.  

Synonyms: The _precision_/_positive predictive value_ is the opposite of FDR, but convey same information:

$$
\mbox{Precision} = \mbox{Positive Predictive Value} = 1 - \mbox{FDR}
$$

So this procedure had a 86% precision.  Among flagged spam e-mails, 86% were actually spam.  


## Who uses these terms?


All these synonyms for the same error rates can be a pain!  But their usage tends to be field-dependent.  

- FPR, FNR, FDR: statistics, machine learning  
- Sensivity, specificity, positive predictive value: medicine, epidemiology, and public health  
- Precision and recall: database and search engine design, machine learning, computational linguistics

Solution: always go back to the confusion matrix!  It tells the whole story.  Ironically, the confusion matrix _avoids confusion_ over terminology.


## ROC curve

In our discussion our these error rates for our spam classifier, we use a threshold of 50%.

- $P(y=1 | x) \geq 0.5 \longrightarrow$ spam
- $P(y=1 | x) < 0.5 \longrightarrow$ not spam

But what if we varied the threshold?  

This is the question addressed by a __ROC curve:__

- a ROC ("receiver operating characteristic")\footnote{This name comes from radar operators in WWII.} curve is a graph showing the performance of a binary classifier at all classification thresholds.  
- at each threshold $t$ we compute both the FPR and the TPR.  
- we then graph TPR(t) versus FPR(t) as t varies.  


## ROC curve for spam classifiers

Let's look at the ROC curve for our two spam classifiers

```{r, echo=FALSE, message=FALSE, fig.width = 4.5, fig.asp = 0.6, fig.align='center', warning=FALSE}
library(foreach)
phat_test_lm_spam = predict(lm_spam1, spamtest, type='response')
phat_test_logit_spam = predict(logit_spam, spamtest, type='response')

thresh_grid = seq(0.95, 0.05, by=-0.005)
roc_curve_spam = foreach(thresh = thresh_grid, .combine='rbind') %do% {
  yhat_test_linear_spam = ifelse(phat_test_lm_spam >= thresh, 1, 0)
  yhat_test_logit_spam = ifelse(phat_test_logit_spam >= thresh, 1, 0)

  # FPR, TPR for linear model
  confusion_out_linear = table(y = spamtest$y, yhat = yhat_test_linear_spam)
  confusion_out_logit = table(y = spamtest$y, yhat = yhat_test_logit_spam)
  out_lin = data.frame(model = "linear",
                       TPR = confusion_out_linear[2,2]/sum(spamtest$y==1),
                       FPR = confusion_out_linear[1,2]/sum(spamtest$y==0))
  out_logit = data.frame(model = "logit",
                       TPR = confusion_out_logit[2,2]/sum(spamtest$y==1),
                       FPR = confusion_out_logit[1,2]/sum(spamtest$y==0))
  
  rbind(out_lin, out_logit)
} %>% as.data.frame()

ggplot(roc_curve_spam) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC curves: linear vs. logit models") +
  theme_bw(base_size = 10)
```



## ROC curve for spam classifiers

Zoomed in for FPR between 0 and 0.2.

```{r, echo=FALSE, message=FALSE, fig.width = 4.5, fig.asp = 0.6, fig.align='center', warning=FALSE}
ggplot(roc_curve_spam) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC curves: linear vs. logit models") +
  theme_bw(base_size = 10) + 
  xlim(0, 0.20)
```


## ROC curve: summary

A ROC curve plots TPR(t) vs. FPR(t) as functions of the classification threshold $t$.  

- Thus in calculus lingo, it is a "parametric representation," where we choose to define the curve's x and y values in terms of another variable (here, the threshold) for simplicity.  
- Lowering the classification threshold classifies more items as positive (y=1).  
- This increases both False Positives and True Positives.  

A ROC curve that is more "up and to the left" represents better performance, i.e. better detection of true positives at a fixed false positive rate.

Some people report the area under the ROC curve (AUC) as [an overall measure of classifier performance.](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)    


## KNN for classification  


Let's see another approach to classification: K-nearest-neighbors.  Super intuitive:  

- Suppose we want to predict the class for some new $x^\star$.  
- Let's ask: what is the most common class for training-set observations around $x^{\star}$?  

We have to measure nearness using some metric, typically Euclidean distance:
$$
d(x, x') = \sqrt{ \sum_{j=1}^p (x_j - x'_j)^2 }
$$  

Remember the importance of scaling your feature variables here!   Typically we use distances scaled by $\mbox{sd}(x_j)$ rather than raw distances.  


## KNN for classification  


```{r, echo=FALSE, fig.width = 4, fig.asp = 0.6, fig.align='center', warning=FALSE}
toy_samp = seq(100, 3000, by=25)
D_toy = spamfit[toy_samp,]
D_toy[,1:9] = scale(D_toy[,1:9])

p1 = ggplot(D_toy) + 
  geom_jitter(aes(x=word.freq.free, y=char.freq.exclamation,
                  color=factor(y)),
              size=2, alpha = 0.5, width = 0.2, height = 0.2) + 
  theme_bw(base_size=10) +
  ylim(c(-1, 2.5)) + xlim(c(-1,2.5)) + 
  labs(color='Spam?') 
p1 + geom_point(data = D_toy[c(3, 106),], size=3,
                mapping = aes(x=word.freq.free, y=char.freq.exclamation))
```


## KNN for classification  


In-class example: classifying glass shards for a recycling center

6 classes:

- WinF: float glass window
- WinNF: non-float window
- Veh: vehicle window
- Con: container (bottles)
- Tabl: tableware
- Head: vehicle headlamp

See `glass.R` on the class website!  


## Limitations of KNN for classification  


Nearest-neighbor classification is simple, but limited.

- There is no good way to choose K.  Train/test splits work, but they are unstable: different data $\longrightarrow$ different K (perhaps _very_ different).    
- The classification can be very sensitive to K.  
- All you get is a classification, with only rough probabilities.  E.g. with $k=5$, all probability estimates are multiple of 20%.   Without accurate probabilities, it is hard to assess misclassification risk.  
- But the basic idea is the same as in logistic regression: Observations with similar $x$'s should be classified similarly.  




## Multinomial logistic regression  


In logistic regression, we get binary class probabilities.  

In multi-class problems, the response is one of $K$ categories.  We'll encode this as $y_i = [0, 0, 1, \ldots, 0]$ where $y_{ik} = 1$ if response $i$ is in class $k \in \{1, \ldots, K\}$.   

In multinomial logistic regression (MLR), we fit a model for 
$$
E(y_{ik} \mid x_i) = P(y_{ik} = 1 \mid x_i) =  g(x_i \cdot \beta_k)
$$
That is, we fit regression coefficients for _each class._  


## Multinomial logistic regression  


In the MLR model, we construct this by analogy with the sigmoid link function (from binary LR) as follows:  
$$
\hat{p}_{ik} = P(y_{ik} = 1 \mid x_i) = \frac{e^{x_i \cdot \beta_k}}{ \sum_{l=1}^K e^{x_i \cdot \beta_l}}  
$$  

I like to think of this as each class vying to predict the outcome for $x_i$ as its own, via a "rate and normalize" procedure:  

- each class "rates" $x_i$ as $e^{x_i \cdot \beta_k}$.  The closer $x_i$ is to the class-specific regression coefficient $\beta_k$, the bigger this rating is.  
- Ratings $\to$ probs: divide by the sum of the ratings across classes.    
- This is often called the "softmax" function.  


## Multinomial logit: glass example    



```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(MASS)
data(fgl) 		## loads the data into R; see help(fgl)
```

\footnotesize
```{r, message=FALSE}
library(nnet)
fgl_split = initial_split(fgl, prop=0.8)
fgl_train = training(fgl_split); fgl_test = testing(fgl_split)
ml1 = multinom(type ~ RI + Mg, data=fgl_train)
coef(ml1) %>% round(2)
```

## Multinomial logit: glass example    



Fitted class probabilities for the first five test-set examples:  
```{r, message=FALSE}
predict(ml1, fgl_test, type='probs') %>%
  head(5) %>%
  round(3)
```

## Multinomial logit: glass example    



How did we do?  Let's look at a confusion matrix:  
\bigskip

\footnotesize
```{r, message=FALSE}
yhat_test = predict(ml1, newdata = fgl_test, type='class')
conf_mat = table(fgl_test$type, yhat_test)
conf_mat

sum(diag(conf_mat))/sum(conf_mat)
```


## Evaluating a classifier


In making decisions, both costs and probabilities matter.  E.g. if $P(y = 1 \mid x) = 0.3$, how would you respond differently if:  

- $x$ is word content of an e-mail and $y$ is spam status?  
- $x$ is mammogram result and $y$ is breast cancer status?  
- $x$ is DNA test and $y$ is guilty/not guilty?  

Different kinds of errors may have different costs.  Thus it helps to de-couple two tasks: _modeling probabilities accurately_ and _making decisions._

This suggests that we evaluate the performance of a classifier in terms its _predicted probabilities_, not its _decisions about class labels._



## Evaluating a classifier: likelihood

The natural way to do us is by calculating the _likelihood_ for our model's predicted probabilities.  Suppose that our classifier produces predicted probabilities $\hat{p}_{ik}$ for each response $i$ and class $k$. Then the likelihood is

$$
\begin{aligned}
\mbox{Like} &= \prod_{i=1}^n \prod_{l=1}^K \hat{p}_{il}^{y_{il}} \\
&= \prod_{i=1}^n \hat{p}_{i, k_i} 
\end{aligned}
$$
where $k_i$ is the observed class label for case $i$.

To get from the first to the second lines, notice that $y_{il} = 1$ for $l=k_i$, and zero otherwise.

## Evaluating a classifier: log likelihood


On a log scale, this becomes
$$
\mbox{loglike} = \sum_{i=1}^n \log \hat{p}_{i, k_i}
$$
In words: we sum up our model's predicted log probabilities for the outcomes $y_{i, k_i}$ that actually happened.  

As with everything in statistical learning: we can calculate an in-sample or a out-of-sample log likelihood, and the out-of-sample is more important!  

Q: what's the largest possible log likelihood for a classifier?    


## Evaluating a classifier: deviance



Sometimes we quote a model's _deviance_ instead of its log likelihood.  The relationship is simple:
$$
\mbox{deviance} = -2 \cdot \mbox{loglike}
$$
Log likelihood measures _fit_ (which we want to maximize), deviance measures _misfit_ (which we want to minimize).  

So the negative sign makes sense.  But why the factor of 2?  _Because of the analogy because least squares and the normal distribution._  


## Evaluating a classifier: deviance



Remember back to an ordinary regression problem with normally distributed errors, $y_i \sim N(f(x_i), \sigma^2)$:  
$$
\mbox{Like} = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{ - \frac{1}{2}(y_i - f(x_i))^2 \right\} 
$$
On a log scale, up to a constant not involving $f(x)$, this becomes:
$$
\mbox{loglike} \propto -\frac{1}{2} \sum_{i=1}^n (y_i - f(x_i))^2 = -\mbox{RSS}/2
$$
where RSS = residual sums of squares.  

Deviance generalizes the notion of "residual sums of squares" to non-Gaussian models.  


## Bayes' Rule for classification


Recall Bayes' rule:
$$
P(A \mid B) = \frac{P(A) P(B \mid A)}{P(B)}
$$

You might remember that each of these terms has a name:

- $P(A)$: the prior probability  
- $P(A \mid B):$ the posterior probability  
- $P(B \mid A)$: the likelihood  
- $P(B)$: the marginal (total/overall) probability  

In classification, "A" is a class label and "B" is a set of features.  

## Bayes' Rule for classification



Bayes's rule:  
$$
P(y = k \mid x) = \frac{P(y = k) \cdot P(x \mid y = k)}{P(x)}
$$

$P(y = k)$ is the prior probability for class $k$.  We usually get this from the raw class frequencies in the training data.  For example:

```{r}
table(fgl_train$type) %>% prop.table %>% round(3)
```


## Bayes' Rule for classification



Bayes's rule:  
$$
P(y = k \mid x) = \frac{P(y = k) \cdot P(x \mid y = k)}{P(x)}
$$

$P(x)$ is the marginal probability of observing feature vector $x$.  Notice it doesn't depend on $k$!  It's the same number for all classes.

Thus we usually write the posterior probabilities up to this constant of proportionality, without bothering to compute it:
$$
P(y = k \mid x) \propto P(y = k) \cdot P(x \mid y = k)
$$
(Note: often we do the actual computations on a log scale instead.)

## Bayes' Rule for classification



Bayes's rule:  
$$
P(y = k \mid x) = \frac{P(y = k) \cdot P(x \mid y = k)}{P(x)}
$$

The hard part is estimating the likelihood $P(x \mid y = k)$.  In words: how likely is it that we would have observed feature vector $x$ if the true class label were $k$?

This is like regression in reverse!  

## Naive Bayes


Recall that $x = (x_1, x_2, \ldots, x_p)$ is a vector of $p$ features.  The simplest strategy for estimating $P(x \mid y = k)$ is called "Naive Bayes."

It's "naive" because we make the simplifying assumption that _every feature $x_j$ is independent_ of all other features, conditional on the class labels:
$$
\begin{aligned}
P(x \mid y = k) &= P(x_{1}, x_{2}, \ldots, x_{p} \mid y = k) \\
&= \prod_{j=1}^p P(x_{j} \mid y = k) \quad \mbox{(independence)}
\end{aligned}
$$

This simplifies the requirements of the problem: _just calculate the marginal distribution of the features,_ i.e. $P(x_{j} \mid y = k)$ for all features $j$ and classes $k$.  

## Naive Bayes: a small example



In `congress109.csv` we have data on all speeches given on the floor of the U.S. Congress during the 109th Congressional Session (January 3, 2005 to January 3, 2007).  

Every row is a set of _phrase counts_ associated with a single representative's speeches across the whole session. $X_{ij}$ = number of times that rep $i$ utter phrase $j$ during a speech.       

The target variable $y \in \mbox{R, D}$ is the party affiliation of the representative.  


## Naive Bayes: a small example

We'll focus on just a few phrases and famous politicians:  

\bigskip

\tiny
```{r}
# read in data
congress109 = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
congress109members = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
```

Focus on a few key phrases and a few famous pols:
```{r}
X_small = dplyr::select(congress109, minimum.wage, war.terror, tax.relief, hurricane.katrina)
X_small[c('John McCain', 'Mike Pence', 'John Kerry', 'Edward Kennedy'),]
```


## Naive Bayes: a small example



Let's look at these counts summed across all members in each party:  

\bigskip

\tiny
```{r}
y = congress109members$party

# Sum phrase counts by party
R_rows = which(y == 'R')
D_rows = which(y == 'D')
colSums(X_small[R_rows,])
colSums(X_small[D_rows,])
```
\normalsize

So we get the sense that some phrases are "more Republican" and some "more Democrat."  


## Naive Bayes: a small example



To make this precise, let's build our Naive Bayes model for a Congressional speech:   

- Imagine that every phrase uttered in a speech is a random sample from a "bag of phrases," where each phrase has its own probability. (_This is the Naive Bayes assumption of independence._)
- Here the bag consists of just four phrases: "minimum wage", "war on terror", "tax relief," and "hurricane katrina".
- Each class (R or D) has its own probability vector associated with the phrases in the bag.  


## Naive Bayes: a small example



We can estimate these probability vectors for each class from the phrase counts in the training data.

For Republicans:

\bigskip

\tiny

```{r}
probhat_R = colSums(X_small[R_rows,])
probhat_R = probhat_R/sum(probhat_R)
probhat_R %>% round(3)
```
\normalsize

And for Democrats:

\bigskip

\tiny
```{r}
probhat_D = colSums(X_small[D_rows,])
probhat_D = probhat_D/sum(probhat_D)
probhat_D %>% round(3)
```



## Naive Bayes: a small example



Let's now look at some particular member of Congress and try to build the likelihood, $P(x \mid y)$, for his or her phrase counts

\bigskip

\tiny
```{r}
X_small['Sheila Jackson-Lee',]
```
\normalsize

Are Sheila Jackon-Lee's phrase counts $x = (11, 15, 3, 66)$ more likely under the Republican or Democrat probability vector?  

## Naive Bayes: a small example


Recall $P(x \mid y = R)$:  

\bigskip

\tiny

```{r, echo=FALSE}
probhat_R %>% round(4)
```
\normalsize

Under this probability vector:  
$$
\begin{aligned}
P(x \mid y = \mbox{R}) &= P(x_1 = 11 \mid y = \mbox{R}) \\
& \times P(x_2 = 15 \mid y = \mbox{R}) \\
& \times P(x_3 = 3 \mid y = \mbox{R}) \\
& \times P(x_4 = 66 \mid y = \mbox{R}) \\
&= (0.1392)^{11} \cdot (0.2860)^{15} \cdot (0.2353)^{3} \cdot (0.3395)^{66} \\
&= 3.765 \times 10^{-51}
\end{aligned}
$$


## Naive Bayes: a small example



Now recall $P(x \mid y = D)$:  

\bigskip

\tiny

```{r, echo=FALSE}
probhat_R %>% round(4)
```
\normalsize

Under this probability vector:  
$$
\begin{aligned}
P(x \mid y = \mbox{D}) &= P(x_1 = 11 \mid y = \mbox{D}) \\
& \times P(x_2 = 15 \mid y = \mbox{D}) \\
& \times P(x_3 = 3 \mid y = \mbox{D}) \\
& \times P(x_4 = 66 \mid y = \mbox{D}) \\
&= (0.3099)^{11} \cdot (0.0958)^{15} \cdot (0.0711)^{3} \cdot (0.5232)^{66} \\
&= 1.293 \times 10^{-43}
\end{aligned}
$$


## Naive Bayes: a small example



These numbers are tiny, so it's much safer to work on a log scale:
$$
\log P(x \mid y = k) = \sum_{j=1}^p x_{j} \log p^{(k)}_{j}
$$
where $p^{(k)}_{j}$ is the jth entry in the probability vector for class $k$.  

```{r}
x_try = X_small['Sheila Jackson-Lee',]
sum(x_try * log(probhat_R))
sum(x_try * log(probhat_D))
```

## Naive Bayes: a small example



Let's use Bayes' rule (posterior $\propto$ prior times likelihood) to put this together with our prior, estimated using the empirical class frequencies:
```{r}
table(y) %>% prop.table %>% round(3)
```

So:
$$
P(R \mid x) \propto 0.539 \cdot (3.765 \times 10^{-51})
$$
and
$$
P(D \mid x) \propto 0.457 \cdot (1.293 \times 10^{-43})
$$

## Naive Bayes: a small example


To actually calculate a posterior, we must turn this into a set of probabilities by normalizing, i.e. dividing by the sum across all classes:  
$$
\begin{aligned}
P(D \mid x) &= \frac{0.457 \cdot (1.293 \times 10^{-43})}{0.457 \cdot (1.293 \times 10^{-43} + 0.539 \cdot (3.765 \times 10^{-51})} \\
& \approx 1
\end{aligned}
$$

So:  

   1. Our model thinks Sheila Jackson-Lee is a Democrat.   
   2. The data completely overwhelm the prior!  This is often the case in Naive Bayes models.  


## Naive Bayes: a bigger example

Let's turn to `congress109_bayes.R` to see a larger example of Naive Bayes classification, where we fit our model with all 1000 phrase counts.    


## Naive Bayes: summary


- Works by directly modeling $P(x \mid y)$, versus $P(y \mid x$).  
- This __regression in reverse__ only works because we assume that each feature in $x$ is independent, given the class labels.  
- Simple and easy to compute, and therefore scalable to very large data sets and classification problems.    
- Unlike a logit model, it works even more with features $P$ than examples $N$.  
- Often too simple: the "naive" assumption of independence really is a drastic simplification.  
- The resulting probabilities are useful for classification purposes, but often not believeable as probabilities.  
- Most useful when the features $x$ are categorical variables (like phrase counts!)  Very common in text analysis.  

